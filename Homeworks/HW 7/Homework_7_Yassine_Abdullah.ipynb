{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c9637f",
   "metadata": {},
   "source": [
    "# CSCI 3202, Spring 2025\n",
    "### Homework 7\n",
    "### Total: 40 pts\n",
    "### Due:  November 7, 2025 by 11:59 pm\n",
    "\n",
    "<br> \n",
    "\n",
    "### Your name: Abdullah Yassine\n",
    "\n",
    "<br> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b82b8",
   "metadata": {},
   "source": [
    "This homework uses the aima python github repository. You should have cloned the aima-python folder for previous assignments. If you haven't or if you have deleted it, clone it now:\n",
    "\n",
    "`git clone https://github.com/aimacode/aima-python.git`\n",
    "\n",
    "Once you have a copy, you will need to either put your homework directly you will need to either copy the the homework notebook into the aima-data directory (not the folder containing the .ipynb file, but the actual .ipynb file) or you will need to copy the .py files needed by this homework from the aima-data into the folder containing the notebook (the .ipynb file).\n",
    "\n",
    "For this assignment, you will need the files:\n",
    "\n",
    "`mdp4e.py`\\\n",
    "`utils4e.py`\\\n",
    "`utils4.py`\n",
    "\n",
    "Note: You should copy the files from the repository, not move them. If you move the files, they won't be in the repository for your next assignment.\n",
    "\n",
    "After you do this if the import statement fails, make sure the homework, mdp4e.py and utils4e.py files are in the same directory, then quit the notebook and reopen it from the File menu.  This resets the import path to the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8e7318b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp4e import *\n",
    "from utils4e import print_table\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c4b6f",
   "metadata": {},
   "source": [
    "In this homework, we will be working with Markov Decision Processes and Policy Iteration. We will solve sequential decision problems, initially on paper (for only 2 states) and then using [mdp](https://github.com/aimacode/aima-python/blob/master/mdp.py) module from [aima-python](https://github.com/aimacode/aima-python) repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af112cd5",
   "metadata": {},
   "source": [
    "### Problem 1: Solve on paper (15 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1b53e",
   "metadata": {},
   "source": [
    "![Screen%20Shot%202023-11-27%20at%204.29.53%20PM.png](./p1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d8c32",
   "metadata": {},
   "source": [
    "#### If the above diagram, isn't showing up for you in the notebook, please refer to p1.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae5d65",
   "metadata": {},
   "source": [
    "In the above diagram, solve for the optimal policy such that max reward is gained.\n",
    "The grid is a 2x2 map, where:\n",
    "- Diamond and fire are the terminal states with rewards(+1) and (-1) respectively\n",
    "- The non-terminal states have arrows in them defining the initial policy for the agent\n",
    "- All non-terminal states have a living reward of -0.05\n",
    "- Transitional probabilities in the policy direction would be 0.7, the directions perpendicular to policy direction would be 0.15 (As shown in figure)\n",
    "\n",
    "The possible actions for the agent at each state would be: UP(^), DOWN(v), LEFT(<) and RIGHT(>)\n",
    "\n",
    "Find the optimal policy that reaps the max reward from the environment given using policy iteration.\n",
    "\n",
    "Consider the number of iterations to be 2, for policy evaluation phase using value iteration method. Finally, display the optimal policy obtained, with arrows in your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b1156",
   "metadata": {},
   "source": [
    "**attach your solutions in the below cell, and add the image in the zip folder**\\\n",
    "LateX is accepted as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c2e28",
   "metadata": {},
   "source": [
    "We label the grid as:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "s^{+} & s^{-} \\\\\n",
    "\\hline\n",
    "s_{1} & s_{2} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "- $ s^{+} $: diamond (terminal, reward $ R = +1 $)  \n",
    "- $ s^{-} $: fire (terminal, reward $ R = -1 $)  \n",
    "- $ s_{1}, s_{2} $: non-terminal, living reward $ R = -0.05 $  \n",
    "- Discount factor $ \\gamma = 1 $\n",
    "\n",
    "The possible actions are:\n",
    "\n",
    "$$\n",
    "A = \\{ \\uparrow, \\downarrow, \\leftarrow, \\rightarrow \\}\n",
    "$$\n",
    "\n",
    "Transition model for a non-terminal state when choosing action $ a $:\n",
    "\n",
    "- $ 0.7 $ probability to move in the direction of $ a $\n",
    "- $ 0.15 $ probability to move in either perpendicular direction\n",
    "- Moving off-grid keeps the agent in the same cell\n",
    "\n",
    "Rewards depend on the **next** state:\n",
    "- Entering $ s^{+} $: $ +1 $\n",
    "- Entering $ s^{-} $: $ -1 $\n",
    "- Entering any non-terminal: $ -0.05 $\n",
    "\n",
    "The initial policy (from the figure) is:\n",
    "\n",
    "$$\n",
    "\\pi_{0}(s_{1}) = \\uparrow, \\qquad \\pi_{0}(s_{2}) = \\rightarrow\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## Policy Evaluation (2 Iterations)\n",
    "\n",
    "The update rule is:\n",
    "\n",
    "$$\n",
    "U_{k+1}(s) =\n",
    "\\begin{cases}\n",
    "R(s), & \\text{if } s \\text{ is terminal} \\\\[4pt]\n",
    "\\displaystyle \\sum_{s'} P(s'|s,\\pi(s)) \\big[ R(s') + \\gamma U_k(s') \\big], & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Initialize $ U_{0}(s) = 0 $ for all states.\n",
    "\n",
    "\n",
    "### **Iteration 1 (using initial policy)**\n",
    "\n",
    "**For $ s_{1} $ with action $ \\uparrow $:**\n",
    "\n",
    "$$\n",
    "P(s^{+}|s_{1}, \\uparrow) = 0.7, \\quad\n",
    "P(s_{1}|s_{1}, \\uparrow) = 0.15, \\quad\n",
    "P(s_{2}|s_{1}, \\uparrow) = 0.15\n",
    "$$\n",
    "\n",
    "$$\n",
    "U_{1}(s_{1}) = 0.7(1) + 0.15(-0.05) + 0.15(-0.05) = 0.685\n",
    "$$\n",
    "\n",
    "\n",
    "**For $ s_{2} $ with action $ \\rightarrow $:**\n",
    "\n",
    "$$\n",
    "P(s_{2}|s_{2}, \\rightarrow) = 0.85, \\quad\n",
    "P(s^{-}|s_{2}, \\rightarrow) = 0.15\n",
    "$$\n",
    "\n",
    "$$\n",
    "U_{1}(s_{2}) = 0.85(-0.05) + 0.15(-1) = -0.1925\n",
    "$$\n",
    "\n",
    "Terminal states:\n",
    "\n",
    "$$\n",
    "U_{1}(s^{+}) = 1, \\qquad U_{1}(s^{-}) = -1\n",
    "$$\n",
    "\n",
    "\n",
    "### Work for Iteration 2 (Initial Policy $ \\pi_0 $)\n",
    "From Iteration 1 we already have:\n",
    "$$\n",
    "U_1(s_1) = 0.685, \\quad\n",
    "U_1(s_2) = -0.1925, \\quad\n",
    "U_1(s^{+}) = 1, \\quad\n",
    "U_1(s^{-}) = -1\n",
    "$$\n",
    "The initial policy is:\n",
    "$$\n",
    "\\pi_0(s_1) = \\uparrow, \\qquad \\pi_0(s_2) = \\rightarrow\n",
    "$$\n",
    "#### 1. Compute $ U_2(s_1) $ under $ \\pi_0 $\n",
    "At $ s_1 $ the action is $ \\uparrow $.\n",
    "Transition probabilities from $ s_1 $:\n",
    "- Up to $ s^{+} $ with probability $ 0.7 $\n",
    "- Left hits a wall → stay in $ s_1 $ with probability $ 0.15 $\n",
    "- Right to $ s_2 $ with probability $ 0.15 $\n",
    "So:\n",
    "$$\n",
    "U_2(s_1)\n",
    "= 0.7 \\big[ R(s^{+}) + U_1(s^{+}) \\big]\n",
    "+ 0.15 \\big[ R(s_1) + U_1(s_1) \\big]\n",
    "+ 0.15 \\big[ R(s_2) + U_1(s_2) \\big]\n",
    "$$\n",
    "Plug in the rewards:\n",
    "- $ R(s^{+}) = 1 $\n",
    "- $ R(s_1) = -0.05 $\n",
    "- $ R(s_2) = -0.05 $\n",
    "So:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "U_2(s_1)\n",
    "&= 0.7 \\big[ 1 + 1 \\big]\n",
    "+ 0.15 \\big[ -0.05 + 0.685 \\big]\n",
    "+ 0.15 \\big[ -0.05 - 0.1925 \\big] \\\\[4pt]\n",
    "&= 0.7 \\cdot 2\n",
    "+ 0.15 \\cdot 0.635\n",
    "+ 0.15 \\cdot (-0.2425) \\\\[4pt]\n",
    "&= 1.4\n",
    "+ 0.09525\n",
    "- 0.036375 \\\\[4pt]\n",
    "&= 1.458875 \\approx 1.4589\n",
    "\\end{aligned}\n",
    "$$\n",
    "#### 2. Compute $ U_2(s_2) $ under $ \\pi_0 $\n",
    "At $ s_2 $ the action is $ \\rightarrow $.\n",
    "Transition probabilities from $ s_2UNDER $:\n",
    "- Right hits a wall → stay in $ s_2 $ with probability $ 0.7 $\n",
    "- Up to $ s^{-} $ with probability $ 0.15 $\n",
    "- Down hits a wall → stay in $ s_2 $ with probability $ 0.15 $\n",
    "So total:\n",
    "- To $ s_2 $ with probability $ 0.85 $\n",
    "- To $ s^{-} $ with probability $ 0.15 $\n",
    "Then:\n",
    "$$\n",
    "U_2(s_2)\n",
    "= 0.85 \\big[ R(s_2) + U_1(s_2) \\big]\n",
    "+ 0.15 \\big[ R(s^{-}) + U_1(s^{-}) \\big]\n",
    "$$\n",
    "Plug in the rewards:\n",
    "- $ R(s_2) = -0.05 $\n",
    "- $ R(s^{-}) = -1 $\n",
    "So:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "U_2(s_2)\n",
    "&= 0.85 \\big[ -0.05 - 0.1925 \\big]\n",
    "+ 0.15 \\big[ -1 - 1 \\big] \\\\[4pt]\n",
    "&= 0.85 \\cdot (-0.2425)\n",
    "+ 0.15 \\cdot (-2) \\\\[4pt]\n",
    "&= -0.206125\n",
    "- 0.3 \\\\[4pt]\n",
    "&= -0.506125 \\approx -0.5061\n",
    "\\end{aligned}\n",
    "$$\n",
    "So after **Iteration 2** under the initial policy $ \\pi_0 $:\n",
    "$$\n",
    "U_2(s_1) \\approx 1.4589, \\qquad U_2(s_2) \\approx -0.5061\n",
    "$$\n",
    "\n",
    "## Policy Improvement\n",
    "Rewards for next states:\n",
    "$$\n",
    "R(s_1) = -0.05, \\quad\n",
    "R(s_2) = -0.05, \\quad\n",
    "R(s^{+}) = 1, \\quad\n",
    "R(s^{-}) = -1\n",
    "$$\n",
    "### At state $ s_1 $\n",
    "#### Action $ \\uparrow $\n",
    "From $ s_1 $ with action $ \\uparrow $:\n",
    "- To $ s^{+} $ with probability $ 0.7 $\n",
    "- To $ s_1 $ (bump into wall on the left) with probability $ 0.15 $\n",
    "- To $ s_2 $ with probability $ 0.15 $\n",
    "So:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(s_1,\\uparrow)\n",
    "&= 0.7 \\big[ R(s^{+}) + U_2(s^{+}) \\big]\n",
    " + 0.15 \\big[ R(s_1) + U_2(s_1) \\big]\n",
    " + 0.15 \\big[ R(s_2) + U_2(s_2) \\big] \\\\[4pt]\n",
    "&= 0.7 \\big[ 1 + 1 \\big]\n",
    " + 0.15 \\big[ -0.05 + 1.4589 \\big]\n",
    " + 0.15 \\big[ -0.05 - 0.5061 \\big] \\\\[4pt]\n",
    "&= 0.7 \\cdot 2\n",
    " + 0.15 \\cdot 1.4089\n",
    " + 0.15 \\cdot (-0.5561) \\\\[4pt]\n",
    "&= 1.4\n",
    " + 0.211335\n",
    " - 0.083415 \\\\[4pt]\n",
    "&\\approx 1.528\n",
    "\\end{aligned}\n",
    "$$\n",
    "#### Action $ \\downarrow $\n",
    "From $ s_1 $ with action $ \\downarrow $:\n",
    "- To $ s_1 $ (off-grid down) with probability $ 0.7 $\n",
    "- To $ s_1 $ (off-grid left) with probability $ 0.15 $\n",
    "- To $ s_2 $ (right) with probability $ 0.15 $\n",
    "So $ P(s_1) = 0.85, \\; P(s_2) = 0.15 $:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(s_1,\\downarrow)\n",
    "&= 0.85 \\big[ R(s_1) + U_2(s_1) \\big]\n",
    " + 0.15 \\big[ R(s_2) + U_2(s_2) \\big] \\\\[4pt]\n",
    "&= 0.85 \\big[ -0.05 + 1.4589 \\big]\n",
    " + 0.15 \\big[ -0.05 - 0.5061 \\big] \\\\[4pt]\n",
    "&= 0.85 \\cdot 1.4089\n",
    " + 0.15 \\cdot (-0.5561) \\\\[4pt]\n",
    "&= 1.197565\n",
    " - 0.083415 \\\\[4pt]\n",
    "&\\approx 1.114\n",
    "\\end{aligned}\n",
    "$$\n",
    "#### Action $ \\leftarrow $\n",
    "From $ s_1 $ with action $ \\leftarrow $:\n",
    "- To $ s_1 $ (off-grid left) with probability $ 0.7 $\n",
    "- To $ s^{+} $ (up) with probability $ 0.15 $\n",
    "- To $ s_1 $ (off-grid down) with probability $ 0.15 $\n",
    "So $ P(s_1) = 0.85, \\; P(s^{+}) = 0.15 $:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(s_1,\\leftarrow)\n",
    "&= 0.85 \\big[ R(s_1) + U_2(s_1) \\big]\n",
    " + 0.15 \\big[ R(s^{+}) + U_2(s^{+}) \\big] \\\\[4pt]\n",
    "&= 0.85 \\big[ -0.05 + 1.4589 \\big]\n",
    " + 0.15 \\big[ 1 + 1 \\big] \\\\[4pt]\n",
    "&= 0.85 \\cdot 1.4089\n",
    " + 0.15 \\cdot 2 \\\\[4pt]\n",
    "&= 1.197565\n",
    " + 0.3 \\\\[4pt]\n",
    "&\\approx 1.498\n",
    "\\end{aligned}\n",
    "$$\n",
    "#### Action $ \\rightarrow $\n",
    "From $ s_1 $ with action $ \\rightarrow $:\n",
    "- To $ s_2 $ with probability $ 0.7 $\n",
    "- To $ s^{+} $ (up) with probability $ 0.15 $\n",
    "- To $ s_1 $ (off-grid down) with probability $ 0.15 $\n",
    "So:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(s_1,\\rightarrow)\n",
    "&= 0.7 \\big[ R(s_2) + U_2(s_2) \\big]\n",
    " + 0.15 \\big[ R(s^{+}) + U_2(s^{+}) \\big]\n",
    " + 0.15 \\big[ R(s_1) + U_2(s_1) \\big] \\\\[4pt]\n",
    "&= 0.7 \\big[ -0.05 - 0.5061 \\big]\n",
    " + 0.15 \\big[ 1 + 1 \\big]\n",
    " + 0.15 \\big[ -0.05 + 1.4589 \\big] \\\\[4pt]\n",
    "&= 0.7 \\cdot (-0.5561)\n",
    " + 0.15 \\cdot 2\n",
    " + 0.15 \\cdot 1.4089 \\\\[4pt]\n",
    "&= -0.38927\n",
    " + 0.3\n",
    " + 0.211335 \\\\[4pt]\n",
    "&\\approx 0.122\n",
    "\\end{aligned}\n",
    "$$\n",
    "So at $ s_1 $:\n",
    "$$\n",
    "Q(s_1,\\uparrow) \\approx 1.528, \\quad\n",
    "Q(s_1,\\downarrow) \\approx 1.114, \\quad\n",
    "Q(s_1,\\leftarrow) \\approx 1.498, \\quad\n",
    "Q(s_1,\\rightarrow) \\approx 0.122\n",
    "$$\n",
    "Best action at $ s_1 $ is **$ \\uparrow $**.\n",
    "---\n",
    "### At state $ s_2 $\n",
    "#### Action $ \\uparrow $\n",
    "From $ s_2 $ with action $ \\uparrow $:\n",
    "- To $ s^{-} $ with probability $ 0.7 $\n",
    "- To $ s_1 $ (left) with probability $ 0.15 $\n",
    "- To $ s_2 $ (off-grid right) with probability $ 0.15 $\n",
    "So:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(s_2,\\uparrow)\n",
    "&= 0.7 \\big[ R(s^{-}) + U_2(s^{-}) \\big]\n",
    " + 0.15 \\big[ R(s_1) + U_2(s_1) \\big]\n",
    " + 0.15 \\big[ R(s_2) + U_2(s_2) \\big] \\\\[4pt]\n",
    "&= 0.7 \\big[ -1 - 1 \\big]\n",
    " + 0.15 \\big[ -0.05 + 1.4589 \\big]\n",
    " + 0.15 \\big[ -0.05 - 0.5061 \\big] \\\\[4pt]\n",
    "&= 0.7 \\cdot (-2)\n",
    " + 0.15 \\cdot 1.4089\n",
    " + 0.15 \\cdot (-0.5561) \\\\[4pt]\n",
    "&= -1.4\n",
    " + 0.211335\n",
    " - 0.083415 \\\\[4pt]\n",
    "&\\approx -1.272\n",
    "\\end{aligned}\n",
    "$$\n",
    "#### Action $ \\downarrow $\n",
    "From $ s_2 $ with action $ \\downarrow $:\n",
    "- To $ s_2 $ (off-grid down) with probability $ 0.7 $\n",
    "- To $ s_1 $ (left) with probability $ 0.15 $\n",
    "- To $ s_2 $ (off-grid right) with probability $ 0.15 $\n",
    "So $ P(s_2) = 0.85, \\; P(s_1) = 0.15 $:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(s_2,\\downarrow)\n",
    "&= 0.85 \\big[ R(s_2) + U_2(s_2) \\big]\n",
    " + 0.15 \\big[ R(s_1) + U_2(s_1) \\big] \\\\[4pt]\n",
    "&= 0.85 \\big[ -0.05 - 0.5061 \\big]\n",
    " + 0.15 \\big[ -0.05 + 1.4589 \\big] \\\\[4pt]\n",
    "&= 0.85 \\cdot (-0.5561)\n",
    " + 0.15 \\cdot 1.4089 \\\\[4pt]\n",
    "&= -0.472685\n",
    " + 0.211335 \\\\[4pt]\n",
    "&\\approx -0.261\n",
    "\\end{aligned}\n",
    "$$\n",
    "#### Action $ \\leftarrow $\n",
    "From $ s_2 $ with action $ \\leftarrow $:\n",
    "- To $ s_1 $ with probability $ 0.7 $\n",
    "- To $ s^{-} $ (up) with probability $ 0.15 $\n",
    "- To $ s_2 $ (off-grid down) with probability $ 0.15 $\n",
    "So:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(s_2,\\leftarrow)\n",
    "&= 0.7 \\big[ R(s_1) + U_2(s_1) \\big]\n",
    " + 0.15 \\big[ R(s^{-}) + U_2(s^{-}) \\big]\n",
    " + 0.15 \\big[ R(s_2) + U_2(s_2) \\big] \\\\[4pt]\n",
    "&= 0.7 \\big[ -0.05 + 1.4589 \\big]\n",
    " + 0.15 \\big[ -1 - 1 \\big]\n",
    " + 0.15 \\big[ -0.05 - 0.5061 \\big] \\\\[4pt]\n",
    "&= 0.7 \\cdot 1.4089\n",
    " + 0.15 \\cdot (-2)\n",
    " + 0.15 \\cdot (-0.5561) \\\\[4pt]\n",
    "&= 0.98623\n",
    " - 0.3\n",
    " - 0.083415 \\\\[4pt]\n",
    "&\\approx 0.603\n",
    "\\end{aligned}\n",
    "$$\n",
    "#### Action $ \\rightarrow $\n",
    "From $ s_2 $ with action $ \\rightarrow $:\n",
    "- To $ s_2 $ (off-grid right) with probability $ 0.7 $\n",
    "- To $ s^{-} $ (up) with probability $ 0.15 $\n",
    "- To $ s_2 $ (off-grid down) with probability $ 0.15 $\n",
    "So $ P(s_2) = 0.85, \\; P(s^{-}) = 0.15 $:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(s_2,\\rightarrow)\n",
    "&= 0.85 \\big[ R(s_2) + U_2(s_2) \\big]\n",
    " + 0.15 \\big[ R(s^{-}) + U_2(s^{-}) \\big] \\\\[4pt]\n",
    "&= 0.85 \\big[ -0.05 - 0.5061 \\big]\n",
    " + 0.15 \\big[ -1 - 1 \\big] \\\\[4pt]\n",
    "&= 0.85 \\cdot (-0.5561)\n",
    " + 0.15 \\cdot (-2) \\\\[4pt]\n",
    "&= -0.472685\n",
    " - 0.3 \\\\[4pt]\n",
    "&\\approx -0.773\n",
    "\\end{aligned}\n",
    "$$\n",
    "So at $ s_2 $:\n",
    "$$\n",
    "Q(s_2,\\uparrow) \\approx -1.272, \\quad\n",
    "Q(s_2,\\downarrow) \\approx -0.261, \\quad\n",
    "Q(s_2,\\leftarrow) \\approx 0.603, \\quad\n",
    "Q(s_2,\\rightarrow) \\approx -0.773\n",
    "$$\n",
    "Best action at $ s_2 $ is **$ \\leftarrow $**.\n",
    "---\n",
    "Therefore the improved policy is:\n",
    "$$\n",
    "\\pi_1(s_1) = \\uparrow, \\qquad \\pi_1(s_2) = \\leftarrow\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302735f",
   "metadata": {},
   "source": [
    "### Problem 2: Solve through code (25 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e575d",
   "metadata": {},
   "source": [
    "![Screen%20Shot%202023-11-27%20at%2010.05.04%20PM.png](./p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec73bbd",
   "metadata": {},
   "source": [
    "#### If the above diagram, isn't showing up for you in the notebook, please refer to p2.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c3078b",
   "metadata": {},
   "source": [
    "**Note: aima-python uses the (column, row) format to denote the squares, thus we will follow the same below.**\n",
    "\n",
    "In the above diagram, solve for the optimal policy such that max reward is gained.\n",
    "The grid is a 5X4 map, where:\n",
    "- Diamond and fire are the terminal states with rewards(+2) and (-2) respectively. \n",
    "- All non-terminal states have a living reward of -0.035\n",
    "- The filled square at (2, 1) denotes a barrier, where it acts like a wall for all its neighboring states (Use `None` in the GridMDP definition)\n",
    "- Transitional probabilities in the policy direction would be 0.8, the directions perpendicular to policy direction would be 0.1 each (Already hard-coded into GridMDP)\n",
    "\n",
    "The possible actions for the agent at any state would be: UP(^), DOWN(v), LEFT(<) and RIGHT(>)\n",
    "\n",
    "Find the optimal policy that reaps the max reward from the environment given using `policy_iteration` method, and display the final policy using the `to_arrows` function from `GridMDP` as shown below. **(5 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f94b166",
   "metadata": {},
   "source": [
    "Also, find the optimal policy when the reward at each non-terminal state is:       **(5 pts each)**\n",
    "- -0.4 \n",
    "- -4 \n",
    "- -0.07 \n",
    "\n",
    "Display the final policy using the `to_arrows` function from `GridMDP` as shown below for each of these.\\\n",
    "Explain briefly how the policy changes and why? **(5 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f3cbc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['class GridMDP(MDP):\\n',\n",
       "  '    \"\"\"A two-dimensional grid MDP, as in [Figure 16.1]. All you have to do is\\n',\n",
       "  '    specify the grid as a list of lists of rewards; use None for an obstacle\\n',\n",
       "  '    (unreachable state). Also, you should specify the terminal states.\\n',\n",
       "  '    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\\n',\n",
       "  '        grid.reverse()  # because we want row 0 on bottom, not on top\\n',\n",
       "  '        reward = {}\\n',\n",
       "  '        states = set()\\n',\n",
       "  '        self.rows = len(grid)\\n',\n",
       "  '        self.cols = len(grid[0])\\n',\n",
       "  '        self.grid = grid\\n',\n",
       "  '        for x in range(self.cols):\\n',\n",
       "  '            for y in range(self.rows):\\n',\n",
       "  '                if grid[y][x]:\\n',\n",
       "  '                    states.add((x, y))\\n',\n",
       "  '                    reward[(x, y)] = grid[y][x]\\n',\n",
       "  '        self.states = states\\n',\n",
       "  '        actlist = orientations\\n',\n",
       "  '        transitions = {}\\n',\n",
       "  '        for s in states:\\n',\n",
       "  '            transitions[s] = {}\\n',\n",
       "  '            for a in actlist:\\n',\n",
       "  '                transitions[s][a] = self.calculate_T(s, a)\\n',\n",
       "  '        MDP.__init__(self, init, actlist=actlist,\\n',\n",
       "  '                     terminals=terminals, transitions=transitions,\\n',\n",
       "  '                     reward=reward, states=states, gamma=gamma)\\n',\n",
       "  '\\n',\n",
       "  '    def calculate_T(self, state, action):\\n',\n",
       "  '        if action:\\n',\n",
       "  '            return [(0.8, self.go(state, action)),\\n',\n",
       "  '                    (0.1, self.go(state, turn_right(action))),\\n',\n",
       "  '                    (0.1, self.go(state, turn_left(action)))]\\n',\n",
       "  '        else:\\n',\n",
       "  '            return [(0.0, state)]\\n',\n",
       "  '\\n',\n",
       "  '    def T(self, state, action):\\n',\n",
       "  '        return self.transitions[state][action] if action else [(0.0, state)]\\n',\n",
       "  '\\n',\n",
       "  '    def go(self, state, direction):\\n',\n",
       "  '        \"\"\"Return the state that results from going in this direction.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '        state1 = tuple(vector_add(state, direction))\\n',\n",
       "  '        return state1 if state1 in self.states else state\\n',\n",
       "  '\\n',\n",
       "  '    def to_grid(self, mapping):\\n',\n",
       "  '        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '        return list(reversed([[mapping.get((x, y), None)\\n',\n",
       "  '                               for x in range(self.cols)]\\n',\n",
       "  '                              for y in range(self.rows)]))\\n',\n",
       "  '\\n',\n",
       "  '    def to_arrows(self, policy):\\n',\n",
       "  \"        chars = {(1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1): 'v', None: '.'}\\n\",\n",
       "  '        return self.to_grid({s: chars[a] for (s, a) in policy.items()})\\n'],\n",
       " 129)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(GridMDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f82923",
   "metadata": {},
   "source": [
    "As you can see in the above code snippet defining GridMDP, the transition probabilities are hard-coded into a variable named `T`, with probability 0.8 in the policy direction and probaility 0.1 each in directions perpendicular to the policy direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "303522af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['def policy_iteration(mdp):\\n',\n",
       "  '    \"\"\"Solve an MDP by policy iteration [Figure 17.7]\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    U = {s: 0 for s in mdp.states}\\n',\n",
       "  '    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\\n',\n",
       "  '    while True:\\n',\n",
       "  '        U = policy_evaluation(pi, U, mdp)\\n',\n",
       "  '        unchanged = True\\n',\n",
       "  '        for s in mdp.states:\\n',\n",
       "  '            a_star = max(mdp.actions(s), key=lambda a: q_value(mdp, s, a, U))\\n',\n",
       "  '            # a = max(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\\n',\n",
       "  '            if q_value(mdp, s, a_star, U) > q_value(mdp, s, pi[s], U):\\n',\n",
       "  '                pi[s] = a_star\\n',\n",
       "  '                unchanged = False\\n',\n",
       "  '        if unchanged:\\n',\n",
       "  '            return pi\\n'],\n",
       " 257)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(policy_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "635a9515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['def policy_evaluation(pi, U, mdp, k=20):\\n',\n",
       "  '    \"\"\"Return an updated utility mapping U from each state in the MDP to its\\n',\n",
       "  '    utility, using an approximation (modified policy iteration).\"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    R, T, gamma = mdp.R, mdp.T, mdp.gamma\\n',\n",
       "  '    for i in range(k):\\n',\n",
       "  '        for s in mdp.states:\\n',\n",
       "  '            U[s] = R(s) + gamma * sum(p * U[s1] for (p, s1) in T(s, pi[s]))\\n',\n",
       "  '    return U\\n'],\n",
       " 275)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(policy_evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a16807b",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "Consider the Problem 1, but transition probailities are 0.8 in policy direction , and 0.1 on both sides perpendicular to policy direction (already hard-coded in aima-python GridMDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37545eaf",
   "metadata": {},
   "source": [
    "#### Solve below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a070d4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Living reward r = -0.035\n",
      ">   >   >      >   .\n",
      "^   ^   >      ^   ^\n",
      "^   .   None   .   ^\n",
      "^   v   >      >   ^\n",
      "\n",
      "Living reward r = -0.4\n",
      ">   >   >      >   .\n",
      ">   >   >      ^   ^\n",
      "^   .   None   .   ^\n",
      "^   >   >      >   ^\n",
      "\n",
      "Living reward r = -4\n",
      "v   v   >      >   .\n",
      "v   v   >      v   ^\n",
      ">   .   None   .   <\n",
      "^   ^   >      ^   ^\n",
      "\n",
      "Living reward r = -0.07\n",
      ">   >   >      >   .\n",
      "^   ^   >      ^   ^\n",
      "^   .   None   .   ^\n",
      "^   >   >      >   ^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def make_grid_mdp(r, gamma=0.9):\n",
    "    grid = [\n",
    "        [r,   r,   r,   r,   2],          \n",
    "        [r,   r,   r,   r,   r],          \n",
    "        [r,  -2,  None, -2,  r],         \n",
    "        [r,   r,   r,   r,   r],          \n",
    "    ]\n",
    "    terminals = [(4, 3), (1, 1), (3, 1)]  \n",
    "    return GridMDP(grid, terminals=terminals, init=(0, 0), gamma=gamma)\n",
    "living_rewards = [-0.035, -0.4, -4, -0.07]\n",
    "for r in living_rewards:\n",
    "    grid = make_grid_mdp(r)\n",
    "    pi = policy_iteration(grid)         \n",
    "    print(f\"Living reward r = {r}\")\n",
    "    print_table(grid.to_arrows(pi))     \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0db730",
   "metadata": {},
   "source": [
    "As the living reward becomes more negative, the agent’s strategy shifts from safe and long-term to risky and short-term.\n",
    "- **r = –0.035:** Small penalty → agent safely navigates around fires toward the diamond (+2), even if the path is longer.  \n",
    "- **r = –0.07:** Slightly higher penalty → still aims for the diamond but takes shorter, more direct routes.  \n",
    "- **r = –0.4:** Larger penalty → prefers faster, riskier paths that may pass near fires to minimize step costs.  \n",
    "- **r = –4:** Extremely high penalty → agent chooses to end quickly, even by moving into a fire (–2), since each extra step costs more than dying.\n",
    "\n",
    "Lower living rewards make the agent value shorter paths and faster termination, shifting from cautious reward-seeking to aggressive loss-minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902f7bb9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
